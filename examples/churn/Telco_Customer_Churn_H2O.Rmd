---
title: "Data science with H2O"
subtitle: "A classification problem: Telco Customer Churn"
author: "A. Ghorbani"
date: "August 27, 2016"
output: html_document
bibliography: bibliography.bib
---

A data science example using H2O ([h2o.ai](http://h2o.ai)) to investigating a classification problem, namely customer churn.

# Introduction

H2O is a open-source machine learning software for big-data. 
It is produced in 2011 by the start-up H2O.ai. 
The performance of H2O allows to do a lot of data science analysis possible through fit hundreds of models, fast prediction of large data and a lot other interesting thing that some of them we will be see in this document.

The problem under study is classification of a telecom company customer into whether they would stay as customer or leave the company.
Many companies are interested to know when, why, which customer they are going to lose.
Acquiring this information they can take action accordingly.

I chose churn analysis here just as an example, any other classification problem can be done the well.

The purpose of this note is to show how this analysis can be done using h2o.

I am going to cover the following analysis:

* prediction of customer churn probability using gradient boosting machine (GBM),
* parameter tuning using Bayesian optimization,
* interpretation of the model:
    + specifying the co-variables that have importance for the analysis,
    + specifying the most important co-variables that is responsible for churn probability of each individual customer,
    + constructing a single decision tree (for interpretation of the black box GBM model),
    + (partial) dependence of churn probabilities on each co-variables.


# Load the required packages

The following packages are necessary for this analysis.

```{r, message=FALSE, warning=FALSE, results='hide'}
#=========================================
# TODO: remove unnecessary packages
#=========================================
library(rpart)	
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(partykit)
library(caret)
library(party)
library(rBayesianOptimization)
require(readr)
require(data.table)
require(reshape2)
library(pROC)
require(ggplot2)

require(h2o)
  
```

# Init H2O (connect to a running H2O cluster)

We need to download h2o package, if we hadn't installed before.
At the time of writing this text the latest version was 3.11.0.3596 and I downloaded from [here](http://s3.amazonaws.com/h2o-release/h2o/master/3596/index.html).
After downloading we need to unzip it.

I usually prefer to run h2o from command line.
For more details on how to start h2o from command line see [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/starting-h2o.html#from-the-command-line).

```{bash eval=FALSE}
> java -jar h2o.jar -nthreads 2 -ice_root ./tmp -port 54321 -name AwesomeCloud
```

Additionally you need to install h2o `R` API package in `R`.
```{r eval=FALSE}
install.packages("path_to_h2o_dir/h2o-3.11.0.3596/R/h2o_3.11.0.3596.tar.gz", repos=NULL, type = "source")
```

Finally use `h2o.init()` to connect the h2o cluster (in our case with only one node).

```{r, message=FALSE, warning=FALSE}
# Connect to h2o cluster
h2o.init(port = 54321, startH2O = FALSE)

h2o.removeAll()
```

# Load the data into h2o

For this example, here, we use the data from  [http://www.dataminingconsultant.com/data/churn.txt](http://www.dataminingconsultant.com/data/churn.txt).
The description of the columns can be found [here](http://www.sgi.com/tech/mlc/db/churn.names).

```{r, message=FALSE, warning=FALSE, results='hide'}
data_frame <- 
  h2o.importFile(
    path              = "http://www.dataminingconsultant.com/data/churn.txt",
    sep               = ",", 
    destination_frame = "data_frame")

```

Let's have a look at the data:
```{r, message=FALSE, warning=FALSE, kable, echo=FALSE}
library(knitr)

tmp.df <- data.frame(a = c(1:4), b = c(1:4))

kable(data_frame[1:5,], digits=2 )

#kable(h2o.summary(data_frame, exact_quantiles = TRUE),  digits=2)

kable(h2o.describe(data_frame), digits = 2)
```

NOTE: The function `kable()` is used just to have s nice table in the output.

The column `Churn?` specifies whether the customer has left the plan or not.
For exact meaning of other columns see [here](http://www.sgi.com/tech/mlc/db/churn.names).

In order to build and assess the model we are going to split the data into *training*, *validation* and *testing* data set.
I assume that we the analysis here is applied to a large data set.
It is not true in this example, we have only `r nrow(data_frame)` number of rows, which is not that big.
In such case it is better to split the data set into only training and testing. For model selection and hyper-parameter tuning cross-validation can be used then. 

Since, I assume the example here will be used for a very large data set.
Otherwise why somebody wants to use H2O ;). 
I will split the data set into training, validation and testing data set with ratio of %50, %25 and %25, respectively.

```{r, message=FALSE, warning=FALSE}
split_df  <- 
  h2o.splitFrame(
    data               = data_frame, 
    ratios             = c(0.5,0.25), 
    destination_frames = c("train_frame", "valid_frame", "test_frame"), 
    seed               = 2016)

train_frame <- split_df[[1]]
valid_frame <- split_df[[2]]
test_frame  <- split_df[[3]]
```


# Building model

First the target variable, which is `Churn?` and the co-variables (the rest of the other variables) are defined.
```{r, message=FALSE, warning=FALSE}
y <- "Churn?"
x <- setdiff(names(data_frame),  c(y))
```


## Bayian Optimization

Each algorithm has number of hyper-parameters to be set, like in case of GBM number of trees or learning rate etc.
Always the question is which values to choose for the parameters. There are different ways to find the optimal values for the parameters, like grid search, random search or other general optimization algorithm.

H2O supports grid search and random search. Grid search is kind of brute force and random search is too crude.
I wished H2O guys implemented Bayesian optimization.
But no worries. There are bunch of Bayesian optimization algorithm in `R`.
On the other hand the heavy lifting part, which is function evaluation (i.e. building the models), will be carried out by H2O.
In the following it shown how we can use `rBaysianOptimisation` package for finding the optimal hyper-parameters.

The function `h2o_bayes` is defined as a wrapper for `h2o.gbm` function, which builds and evaluates a GBM model.
All the parameters that needs to be optimized are defined as an argument.
In order to score each model here *validation* data set is used.
However, for small data set in this example usually it is better to use cross-validation (instead of validation data set) to decrease variation in the model score.

```{r, message=FALSE, warning=FALSE}
#============================
# Define the wrapper function
#============================
h2o_bayes <- function(
  max_depth, learn_rate, sample_rate, 
  col_sample_rate, balance_classes){
  bal.cl <- as.logical(balance_classes)
  gbm <- h2o.gbm(  
    x                   = x,
    y                   = y,
    training_frame      = train_frame,
    validation_frame    = valid_frame,
    #nfolds              = 3,
    ntrees              = 900,
    max_depth           = max_depth,
    learn_rate          = learn_rate,
    sample_rate         = sample_rate,
    col_sample_rate     = col_sample_rate,
    score_tree_interval = 5,
    stopping_rounds     = 2,
    stopping_metric     = "logloss",
    stopping_tolerance  = 0.005,
    balance_classes     = bal.cl)
    
  score <- h2o.auc(gbm, valid = T)
  list(Score = score,
       Pred  = 0)
}

#============================
# Find optimal values for the 
# parameters in the given range. 
#============================
OPT_Res <- BayesianOptimization(
  h2o_bayes,
  bounds = list(
    max_depth   = c(2L, 8L), 
    learn_rate  = c(1e-4, 0.2),
    sample_rate = c(0.4, 1), 
    col_sample_rate = c(0.4, 1), 
    balance_classes = c(0L, 1L)),
  init_points = 3,  n_iter = 3,
  acq = "ucb", kappa = 2.576, eps = 0.0,
  verbose = FALSE)
```

## Building the model using optimal parameters

After searching for the best parameters using Bayesian optimization algorithm, we use the values found above to train a GBM model.
```{r, message=FALSE, warning=FALSE, results='hide'}
gbm <- h2o.gbm(
  x                   = x,
  y                   = y,
  training_frame      = train_frame,
  validation_frame    = valid_frame,
  ntrees              = 900,
  max_depth           = OPT_Res$Best_Par["max_depth"],
  learn_rate          = OPT_Res$Best_Par["learn_rate"],
  sample_rate         = OPT_Res$Best_Par["sample_rate"],
  col_sample_rate     = OPT_Res$Best_Par["col_sample_rate"],
  balance_classes     = as.logical(OPT_Res$Best_Par["balance_classes"]),
  score_tree_interval = 5,
  stopping_rounds     = 2,
  stopping_metric     = "logloss",
  stopping_tolerance  = 0.005,
  model_id         = "my_awesome_GBM")

```

## Train a model using only important co-variables

In the previous try to train a GBM model I have used all the columns in the data.
Many times removing irrelevant data from the training data will improves the results for various reason.
Even if it doesn't improve the result, it will reduce the amount of resources needed for calculation, e.g. CPU time and memory.

Hence, a model is trained with only important variables that can be obtained from previous calculation.

```{r, message=FALSE, warning=FALSE, results='hide'}

var.imp <- h2o.varimp(gbm)[h2o.varimp(gbm)$scaled_importance > 0.01, "variable"]
# The value of 0.01 is arbitrary, you might want to use other values.

gbm_varImp <- h2o.gbm(
  x                   = var.imp,
  y                   = y,
  training_frame      = train_frame,
  validation_frame    = valid_frame,
  ntrees              = 900,
  max_depth           = OPT_Res$Best_Par["max_depth"],
  learn_rate          = OPT_Res$Best_Par["learn_rate"],
  sample_rate         = OPT_Res$Best_Par["sample_rate"],
  col_sample_rate     = OPT_Res$Best_Par["col_sample_rate"],
  balance_classes     = as.logical(OPT_Res$Best_Par["balance_classes"]),
  score_tree_interval = 5,
  stopping_rounds     = 2,
  stopping_metric     = "logloss",
  stopping_tolerance  = 0.005,
  model_id         = "my_awesome_GBM_varImp")

```

## Compare the two model

The plots below the tow models are compared.
It is clearly visible that if we use all the co-variables no significant improvement is achieved.

```{r, message=FALSE, warning=FALSE}
shist <- gbm@model$scoring_history[, c("duration", "validation_rmse", "validation_auc")]
shist$algorithm <- "GBM" 
scoring_history <- shist

shist <- gbm_varImp@model$scoring_history[, c("duration", "validation_rmse", "validation_auc")]
shist$algorithm <- "GBM with var.imp." 
scoring_history <- rbind(scoring_history,shist)

scoring_history$duration <- as.numeric(
  gsub("sec", "", scoring_history$duration))

scoring_history <- melt(scoring_history, id = c("duration", "algorithm"))

ggplot(data = scoring_history, 
       aes(x     = duration, 
           y     = value, 
           color = algorithm,
           group = algorithm)) + 
  geom_line() + geom_point() +
  facet_grid(. ~ variable, scales = "free",shrink = TRUE,space = "free")

```


Choose the best model based on `AUC` measure (you could choose `mse`, `logloss` or other measures).
```{r, message=FALSE, warning=FALSE}

AUC_gbm        <- h2o.performance(gbm, valid = T)@metrics$AUC
AUC_gbm_varImp <- h2o.performance(gbm_varImp, valid = T)@metrics$AUC
if(AUC_gbm > AUC_gbm_varImp){
  bestModel <- gbm
}else{
  bestModel <- gbm_varImp
}
cat("The best model is '", bestModel@model_id, 
    "' with AUC of ", max(AUC_gbm, AUC_gbm_varImp), 
    " vs ",  min(AUC_gbm, AUC_gbm_varImp), "\n" )

```

# Model Assessment

Now we use the test data to measure the performance of the model:
```{r, message=FALSE, warning=FALSE}
bestPerf <- h2o.performance(bestModel, test_frame)

perfDF <- melt(as.data.frame(bestPerf@metrics$thresholds_and_metric_scores), 
           id = "threshold")
```

Performance of the model on the test data:
```{r, message=FALSE, warning=FALSE, kable2}

kable(
  as.data.frame(
    bestPerf@metrics[c("MSE", "RMSE", "AUC", "r2", "logloss", "Gini", "mean_per_class_error")]), 
  digits = 3)

```

As you know the prediction of the model is a probability of a customer is going to leave or not.

Lets have a look at the prediction distribution:
```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
pred <- h2o.predict(bestModel, test_frame)
pred.df <- as.data.frame(h2o.cbind(test_frame[, y], pred))

score_table <- bestPerf@metrics$max_criteria_and_metric_scores
best_acc_thresh <- score_table[score_table$metric == "max accuracy", "threshold"]

source("https://raw.githubusercontent.com/a-ghorbani/notebooks/master/H2O/examples/plot_prediction_distribution.R")

p <- plot_prediction_distribution( 
  probs = pred.df$True.,
  truth = as.factor(as.numeric(pred.df$Churn. == "True.")),
  threshold = best_acc_thresh)
  
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
p
```

We would like to have all data points be either in top right (True Positive (TP)) or bottom left (True Negative (TN)).

So, which customer is going to leave?
If the prediction value is more than 0.5? well it depends on our criteria.
Here are some of the criteria and the maximum value we can get depending on which criteria we choose: 
```{r, message=FALSE, warning=FALSE, kable3}

kable(bestPerf@metrics$max_criteria_and_metric_scores, digits = 3)

```

and here are the plots of few of them.
```{r, message=FALSE, warning=FALSE, kable4}
scores_to_plot <- c("accuracy", "precision", "recall", "min_per_class_accuracy")

ggplot(data = perfDF[perfDF$variable %in% scores_to_plot, ],  
       aes(x     = threshold, 
           y     = value, 
           color = variable,
           group = variable)) + 
  geom_line() + geom_point() 

#kable(bestPerf@metrics, digits = 3)

```

If we know the actual costs (in the business) of false positives and false negatives (or benefits of true positives) we can have better decision on which criteria to choose.

# Interpretation of the Model

Up to now we built the model and evaluated. 
The algorithm that we used above was GBM, which is a black box model.
That means it only gives us prediction on which customers are going to leave, i.e. the probabilities.

But we would like to have some more insights and be able to answer questions like why a customer wants to leave.

## Variable Importance

In many situations not all the co-variables are equally important. Some variable are more important that the others. Using variable importance we can focus more on them. 
```{r}

h2o.varimp_plot(bestModel)

```

## Variable Importance per Customer

The above mentioned variable importance were measured based on the whole training data set and is only valid on average, not each individual customer.

We can use the trained model to approximate the variable importance based on each individual customer.
In this approach a prediction is done on the data set. 
And then we leave a column out and will do the prediction again and measure the difference in probabilities.
The difference in probabilities in original predictions compared the one the a variable is removed can indicate the importance of that variable for each customer.

This have a little bit of calculation in `R` side, like calculating differences and ranking each row based on these differences.
Hence, in order to speed up thing I have implemented a C++ code which also needed to be *sourced*.
Using `Rcpp` package one can simply use C++ code in `R` script. 

```{r message=FALSE, warning=FALSE, results='hide'}
require(Rcpp)
Rcpp::sourceCpp("get_high_rank_values.cpp")
source("https://raw.githubusercontent.com/a-ghorbani/notebooks/master/H2O/examples/churn/rowVarImp.R")

df <- rowVarImp.h2o(model=bestModel, cols=var.imp, data_frame=test_frame, n=3)

```

In here only first 3 importance variable for each customer are calculated.
The first 3 columns shows the differences by not including the variable.
The next 3 columns shows their values and the last 3 columns shows the variables names.

Knowing these information the business unit can take measures based on each individual customer.

```{r message=FALSE, warning=FALSE}
kable(
  head(df),
  digits=3
)

```

## Partial Dependence Plots 
```{r message=FALSE, warning=FALSE, results='hide'}
source("https://raw.githubusercontent.com/a-ghorbani/notebooks/master/H2O/examples/churn/partialDependencePlot.R")

cols <- var.imp[1:5]
pdps <- lapply(cols, function(x) partialDependencePlot(bestModel, x, data_frame) )

lapply(1:length(cols), function(x){
  x_ <- melt(pdps[[x]])
  colnames(x_) <- c("variable", "diff")
  p <- ggplot(data = x_, aes(x = variable, y = diff))  
  if(class(x_$variable) == 'factor'){
    p <- p + geom_bar(stat="identity")
  }else{
    p <- p + geom_line()
  }
  p + ggtitle(cols[x])
})

```

```{r message=FALSE, warning=FALSE, results='hide'}
cols <- c("State", "Day Charge")
pdp1 <- partialDependencePlot(bestModel, cols, data_frame)

pdp2 <- partialDependencePlot(bestModel, "Day Charge", data_frame)

pdp1 <- melt(pdp1)
pdp2 <- melt(pdp2)
pdp2$State <- "AVG"

colnames(pdp1) <- c("State", "Day.Charge", "diff")
colnames(pdp2) <- c("Day.Charge", "diff", "State")

```

```{r message=FALSE, warning=FALSE}
ggplot(data = pdp1, 
       aes(x     = Day.Charge, 
           y     = diff,   
           color = State,
           group = State)) + 
  geom_line() +
  geom_line(data = pdp2, size=3, color='black')
```

## A Simplified Single Decision Tree

```{r message=FALSE, warning=FALSE, results='hide'}
denoise.h2o.df <- function(model, data_frame, destination_frame, lower.q=0.2, higher.q=0.8){
  # Removes the data points that cannot be classified significantly.
  # That means the data points for which the prediction is far away from 0 and 1, 
  # like those close to 0.5
  #
  # Args:
  #   model: A H2O model.
  #   data_frame: A H2O data frame.
  #   destination_frame: ID for the result frame.
  #   lower.q: The lower quantile, from which the data points will be ignored.
  #   higher.q: The higher quantile, to which the data points will be ignored.
  #
  # Returns:
  #   The resulting H2O data frame.
  
  pred   <- h2o.predict(model, data_frame)
  predCol <- names(pred)[3]
  quant  <- h2o.quantile(pred[,predCol], probs = c(lower.q, higher.q))
  lower  <- min(quant)
  higher <- max(quant)
  data_pred <- h2o.cbind(data_frame, pred)
  data_denoised <- data_pred[
    data_pred[,predCol] < lower | 
      data_pred[,predCol] > higher,]
  data_denoised <- h2o.assign(data_denoised, destination_frame) 
  return(data_denoised)  
}

denoised_train_df <- 
  denoise.h2o.df(
    model             = bestModel, 
    data_frame        = train_frame, 
    destination_frame = "denoised_train_frame", 
    lower.q           = 0.1,
    higher.q          = 0.9)

denoised_test_df <- 
  denoise.h2o.df(
    model             = bestModel, 
    data_frame        = test_frame, 
    destination_frame = "denoised_test_frame", 
    lower.q           = 0.1,
    higher.q          = 0.9)

```

```{r}
train_df <- as.data.frame(denoised_train_df)
test_df  <- as.data.frame(test_frame)
test_df1 <- as.data.frame(denoised_test_df)

# Ignore the columns that is not in used in training 
tmp.df <- train_df[, -which(names(train_df)  %in% c("Churn.","False.","True.", "Phone" ))]
colnames(tmp.df)[colnames(tmp.df) == "predict"] <- "Churn."

counts <- as.data.frame(h2o.table(data_frame[,"Churn?"]))

priorDist <- counts$Count[1] / sum(counts$Count)
priorDist <- c(priorDist, 1 - priorDist)
               
tree <- rpart(
    formula  = Churn. ~ ., 
    data     = tmp.df,
    maxdepth = 5, 
    cp       = 0.02,
    parms    = list(prior = priorDist, 
                    split = "information"))

fancyRpartPlot(tree)

id <- which(!(test_df$State %in% levels(train_df$State)))
test_df$State[id] <- NA

Pred1 <- predict(tree, test_df)
test_df$Prediction1 <- Pred1[,"True."]
ROC1 <- roc(test_df$Churn., test_df$Prediction1)
ROC1$auc
```